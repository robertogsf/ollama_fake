# Ollama con IA Real

Este proyecto simula la API de Ollama pero usando modelos de IA reales con Hugging Face Transformers, incluyendo el modelo **Qwen2.5:14B**.

## ğŸ¯ CaracterÃ­sticas

- âœ… API compatible con Ollama
- ğŸ¤– Modelos de IA reales (Qwen2.5, Llama3.2)
- ğŸ”„ Streaming de respuestas
- ğŸ“Š Endpoints completos (/api/generate, /api/chat, /api/tags, etc.)
- ğŸ® Descarga automÃ¡tica de modelos
- ğŸ”§ GestiÃ³n inteligente de memoria GPU/CPU

## ğŸš€ InstalaciÃ³n RÃ¡pida

```bash
# Clonar y configurar
cd ollama_con_poderes
./setup.sh

# Activar entorno
source venv/bin/activate

# Iniciar servidor
python server_real.py
```

## ğŸ¤– Modelos Disponibles

| Modelo | TamaÃ±o | DescripciÃ³n |
|--------|--------|-------------|
| `qwen2.5:14b` | ~28GB | Qwen2.5 14B Instruct (Principal) |
| `qwen2.5:7b` | ~14GB | Qwen2.5 7B Instruct (Recomendado) |
| `qwen2.5:3b` | ~6GB | Qwen2.5 3B Instruct (Eficiente) |
| `llama3.2:3b` | ~6GB | Llama 3.2 3B Instruct |

## ğŸ“¡ Endpoints API

### Listar modelos
```bash
curl http://localhost:11434/api/tags
```

### Generar texto
```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:7b",
    "prompt": "Â¿CuÃ¡l es la capital de EspaÃ±a?",
    "stream": false
  }'
```

### Chat
```bash
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:7b",
    "messages": [
      {"role": "user", "content": "Hola, Â¿cÃ³mo estÃ¡s?"}
    ],
    "stream": false
  }'
```

### Descargar modelo
```bash
curl -X POST http://localhost:11434/api/pull \
  -H "Content-Type: application/json" \
  -d '{"name": "qwen2.5:7b"}'
```

## ğŸ§ª Pruebas

```bash
# Ejecutar suite de pruebas
python test.py

# Probar modelo especÃ­fico
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:14b", "prompt": "Escribe un poema corto"}'
```

## âš™ï¸ ConfiguraciÃ³n

Edita `.env` para personalizar:

```env
# Puerto del servidor
PORT=11434

# Modelo por defecto
DEFAULT_MODEL=qwen2.5:7b

# Cache de modelos
TRANSFORMERS_CACHE=/home/models/cache
HF_HOME=/home/models/hf_cache
```

## ğŸ”§ Uso con n8n

1. Configura n8n para usar `http://localhost:11434` como URL de Ollama
2. Los modelos aparecerÃ¡n como `qwen2.5:7b`, `qwen2.5:14b`, etc.
3. El primer uso descargarÃ¡ el modelo automÃ¡ticamente

## ğŸ“‹ Requisitos del Sistema

- **RAM**: MÃ­nimo 16GB (32GB recomendado para modelos grandes)
- **GPU**: NVIDIA con CUDA (opcional pero recomendado)
- **Espacio**: 30-50GB para cachÃ© de modelos
- **Python**: 3.8+

## ğŸš¨ Notas Importantes

- â³ **Primera ejecuciÃ³n**: Descarga del modelo puede tomar 10-30 minutos
- ğŸ’¾ **Memoria**: Modelos grandes requieren mucha RAM/VRAM
- ğŸ”„ **Cambio de modelo**: Descarga automÃ¡ticamente el modelo nuevo
- ğŸ—‘ï¸ **Limpieza**: Usa `/api/models/unload` para liberar memoria

## ğŸ› Troubleshooting

### Error de memoria
```bash
# Usar modelo mÃ¡s pequeÃ±o
curl -X POST http://localhost:11434/api/generate \
  -d '{"model": "qwen2.5:3b", "prompt": "test"}'

# Descargar modelo actual
curl -X POST http://localhost:11434/api/models/unload
```

### Error de CUDA
```bash
# Forzar uso de CPU en model_config.py
DEVICE_CONFIG = {
    "auto_device_map": False,  # Cambiar a False
    ...
}
```

## ğŸ“ˆ Monitoreo

```bash
# Health check
curl http://localhost:11434/health

# Estado del servidor
curl http://localhost:11434/version
```

## ğŸ”— IntegraciÃ³n con Docker

Para usar en Docker con n8n, expone el puerto 11434 y configura las variables de entorno apropiadas.

---

**Â¡Disfruta de tu IA real con compatibilidad Ollama!** ğŸ‰
